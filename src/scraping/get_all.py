import logging
import re
from src.saving.db_writer import db_writer_results
from src.scraping.get_match_results import get_matches_from_url


def get_all(url_start, category):
    """
    Logique principale : Scrape toutes les journÃ©es d'une poule donnÃ©e.
    """
    logger = logging.getLogger(__name__)
    logger.info(f"ğŸš€ DÃ©marrage scraping '{category}' via Requests")

    all_match_data = []

    # 1. Scraping de la page initiale (donnÃ©e en config)
    logger.info(f"Traitement URL initiale : {url_start}")
    matches, journees_meta = get_matches_from_url(url_start, category)

    if matches:
        all_match_data.extend(matches)
        logger.info(f"  -> {len(matches)} matchs trouvÃ©s sur la page initiale.")

    # 2. Gestion de la pagination intelligente
    if journees_meta:
        logger.info(f"  -> {len(journees_meta)} journÃ©es dÃ©tectÃ©es dans la structure.")
        if len(journees_meta) > 0:
            first_j = journees_meta[0]
            logger.info(f"  ğŸ” [DEBUG STRUCT] Keys disponibles dans journees_meta[0]: {list(first_j.keys())}")

        base_url_pattern = re.sub(r"journee-\d+/?", "journee-{}/", url_start)

        if base_url_pattern == url_start:
            if not base_url_pattern.endswith("/"): base_url_pattern += "/"
            base_url_pattern += "journee-{}/"

        count_paginated = 0
        for journee in journees_meta:
            num = journee.get("journeeNumero") or journee.get("journee_numero") or journee.get("numero")

            if not num:
                logger.warning(f"  âš ï¸ Impossible de trouver le numÃ©ro de journÃ©e dans : {journee}. Skip.")
                continue

            if f"journee-{num}" in url_start or f"journee-{num}/" in url_start:
                continue

            target_url = base_url_pattern.format(num)

            page_matches, _ = get_matches_from_url(target_url, category)
            if page_matches:
                all_match_data.extend(page_matches)
                count_paginated += len(page_matches)

        logger.info(f"  -> {count_paginated} matchs supplÃ©mentaires rÃ©cupÃ©rÃ©s via pagination.")

    else:
        logger.warning(
            "âš ï¸ Impossible de dÃ©tecter les autres journÃ©es automatiquement. Seule l'URL fournie a Ã©tÃ© traitÃ©e.")

    # 3. Sauvegarde Atomique
    logger.info(f"ğŸ Fin du scraping pour '{category}'. Total: {len(all_match_data)} matchs. Ã‰criture BDD...")
    db_writer_results(all_match_data, category)